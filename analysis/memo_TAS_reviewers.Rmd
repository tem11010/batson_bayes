---
output: 
  pdf_document:
    citation_package: biblatex
    keep_tex: false
    latex_engine: pdflatex
    template: svm-latex-memo.tex
    number_sections: true
fontfamily: mathpazo
fontsize: 11pt
geometry: margin=1in
header-includes:
   - \linespread{1.05}

from: Authors
to: The American Statistican
subject: Reply to Reviewer Comments
date: "`r format(Sys.Date(), '%B %d, %Y')`"
memorandum: true
graphics: false
width: .3
bibliography: references.bib
---

This memorandum replies to comments provided by the TAS associate editor and two reviewers. We thank the editor and reviewers for their comments. We believe the revised paper is much better as a result.

# Key Issues

Here, we address several key issues raised by multiple reviewers.

1. As our revised Introduction now makes clear (p. 4-5 and Fig. 1), neither our statistical approach (nor any other) can alone detect _Batson_ violations, because _Batson_ procedure permits any type of admissible evidence of illegal strike bias -- not just the attorney's past strike data -- and requires the trial judge to weigh all such evidence to decide whether illegal strike bias exists. 

    a. For example, _Batson_ procedure requires the striking attorney at a certain stage to justify the challenged strikes based on (non-discriminatory) reasons. If the attorney fails to produce such reasons, the trial judge must find a _Batson_ violation. If the attorney offers such reasons, the trial judge must assess evidence as to the credibility of those reasons. Our statistical approach cannot adjust the posterior distribution of the bias parameter to account for the credibility of those reasons or other types of evidence (other than the attorney's pattern of strikes) of illegal strike bias. Rather, our approach helps attorneys evaluate the inference of illegal bias in the present case based on historical strike data but _before_ accounting for other evidence relevant to illegal strike bias. 
  
    b. The phrase "bias detection" in the paper means only that the credible interval for the bias parameter excludes zero, given the historical strike data, the model, and choice of $\alpha$ value of the power prior. For that "detection" task, our approach lets one assign at most equal weight to past strikes and their strikes in the present case ($\alpha = 1$). In turn, attorneys can use the resulting bias estimate, combined with other evidence, to (1) decide whether to argue _Batson_, (2) to prove the _Batson_ prima facie case, and (3) to persuade the trial judge to find  a _Batson_ violation (i.e., illegal strike bias is more likely than not, given all the evidence).

2. Possible incompatibility of $b_{hist}$ and $b_{curr}$ is always an issue when using past strike data to infer strike bias in the current trial. This is the case for _any_ statistical approach that uses past strike data to infer strike bias in the current trial. And in _Batson_ challenges involving past strike data, the lawyers are already and always arguing about how much weight to assign to that past strike data, albeit with words. By using the power prior, our approach simply makes the weighting of historical strike data more explicit and transparent as a choice of a value for $\alpha$. The more sensitive bias "detection" (i.e. whether the posterior estimate of $b_{curr}$ excludes zero) is to the choice of $\alpha$, the more one should worry about incompatibility. Thus, small vs. large values of $\alpha$ should vary with the suspected degree of incompatibility. In reality, because $b_{curr}$ and $b_{hist}$ are not directly observable, one will need to rely on background knowledge or assumptions about the degree of incompatibility to decide which value of $\alpha$ to choose. In a _Batson_ challenge, the lawyers must disclose and justify their choice of $\alpha$ accordingly. We have revised the paper's Introduction and Discussion (sec 3.1) to emphasize this.

3. In the adversarial setting of the courtroom, our approach thwarts the strategic selection of $\alpha$ values, because it makes the choice of $\alpha$ transparent, and thus the choice of how much to weigh the historical strike data.  As a result, every lawyer will know that any strategic selection of $\alpha$ will be met with scrutiny from the lawyers on the other side. Absent this transparency, lawyers will continue to argue strategically about how much to weigh historical strike data, except with the opacity of words.  We have revised the paper to add this to the Discussion section (sec. 3.1).

4. Absent some very strong assumptions, there is no generally justifiable hyper prior for $\alpha$ in this context of strike bias and _Batson_ challenges. This is because the proper weight to place on historical data depends on what causes incompatibility between the historical data and the current data. The plausible reasons for non-random missingness and such confounding, however, likely vary considerably depending on strike procedure, courtroom practices, prosecutor charging practices, among other things. As we now note in the Discussion section of the paper, the literature on the power prior discusses information criteria for guide values of $\alpha$ as well as treating $\alpha$ as random and assigning a prior to it. However, that literature is unclear as to what implies for _Batson_ and peremptory strikes, for at least three reasons.  First, that literature has mostly focused on clinical trials and models whose properties are well known (e.g., the linear model), as distinct from the models of strike procedure discussed here. In the clinical trial setting, it might also be easier to capture(measure?) trial-level (and patient-level) characteristics that may be responsible for incompatibility. Even in these settings, selecting a prior for $\alpha$ is not straightforward. @Neelon2010 warn that using random power parameters (in their case, with a beta prior distribution) tends to attenuate the influence of the historical data, and so one might have to use highly informative priors on the power parameter. These authors suggest that practitioners consider whether "it is more appropriate to assign $\alpha$ a fixed value based on expert opinion about the relevance of the historical data to the current analysis". More recent versions of the power prior are beginning to address this issue, for example using "commensurate power priors" (@Hobbs2011).   Second, our approach does not attempt to maximize coverage in the traditional sense, but aims only to "detect" bias by determining whether zero falls outside the chosen credible interval for the posterior distribution of the bias parameter. Third, whereas some papers treat incompatibility as itself a parameter [e.g., @Ollier2020], the precise approaches rely on strong assumptions that do not obviously make sense in our context.  Accordingly, we believe the most conservative choice (for now) is to keep $\alpha$ a fixed value and check for sensitivity of results to the choice of $\alpha$ value. Proceeding otherwise would require the work of a completely new paper.

5. Choosing a credible interval amounts to deciding how conservative one wishes to be about inferring bias from past strike data, because we define bias "detection" narrowly to mean when the posterior for the bias parameter falls outside the chosen credible interval. Thus, the larger the credible interval one chooses, the greater the chance that zero falls within it. For clarity, we now focus on one credible interval (95%) in the paper and have placed simulation results based on other credible intervals in the Appendix. 

6. When we first submitted the paper, we offered to make the app prototype available to the reviewers (as an R package), so they could follow along with section 2.5 of the paper. We received no response to this offer. The offer remains.

In the following responses to particular comments, we will refer to the above items when appropriate.

# Comments of TAS Associate Editor

_[T]he article doesn’t compellingly make the case that the methodology is more useful than simply tracking those who have been reprimanded in the past or that it’s ethical/reasonable to actually employ in the courtroom. Specifically, the way that prior information is encoded, the logic behind a detection appears to be circular in that if someone has been reprimanded in the past they should be reprimanded in the future (in some cases for no reason). This issue hasn’t been discussed to a level that makes the utility of such a model clear._

We have four replies to this concern.

First, the law of _Batson_ and parallel State law has consistently permitted judges to infer illegal strike bias based in part on an attorney's use of strikes in past cases. In reviewing the kinds of evidence that can prove a _Batson_ violation by a prosecutor, the U.S. Supreme Court included, as examples of types of evidence, the "relevant history of the State’s peremptory strikes in past cases." _Flowers v. Mississippi_, 139 S. Ct. 2228, 2243 (2019). State law variants on _Batson_ are also explicit about this. For example, Washington State law provides that in deciding whether "an objective observer could view race or ethnicity as a factor in the use of the peremptory challenge," the circumstances the court "should" consider include "whether the party has used peremptory challenges disproportionately against a given race or ethnicity, in the present case _or in past cases_." Wash. General Rule 37(g)(v) (emphasis added). California law similarly identifies, as a circumstance a court may consider, whether the striking party "has used peremptory challenges disproportionately against a given race . . . in the present case _or in past cases_." Calif. Code of Civil Procedure 231.7(d)(3)(G) (emphasis added).

We hope this removes the worry that it is not "ethical/reasonable" to infer present strike bias from strike behavior in past cases. 

Second, as the phrase "should be reprimanded in the future (in some cases for no reason)" implies, the apparent worry is that our approach will cause judges to infer, in any particular case, that illegal strike bias is more likely than not (i.e., find _Batson_ violations) when in fact it was less likely than not. This is not the case. Our approach does not and cannot automate findings of _Batson_ violations. See Item 1, above.

Third, we assume the word "reprimanded" refers to a court finding of a _Batson_ violation. Our approach, however, relies on an attorney's use of strikes of past cases, regardless of whether those strikes were ever the subject of a _Batson_ challenge. This is better than just "tracking those who have been reprimanded in the past" for _Batson_ violations, because the probability that a court finds a _Batson_ violation depends on an attorney having raised a _Batson_ challenge in the first place. (Absent an attorney arguing _Batson_, a court will not inquire into whether illegal strike bias existed.) If illegal strike bias exists but the attorney erroneously perceives no credible basis for it, the attorney will tend not to argue _Batson_. In cases where there no illegal strike bias but the attorney erroneously perceives some credible basis for it, the attorney will tend to argue _Batson_ but may not persuade the trial judge that illegal strike bias was more likely than not. By relying on past strike data, not whether anyone argued _Batson_ in those past cases, we avoid any bias associated with relying only on past court findings of _Batson_ violations.

Fourth, the comment is overinclusive. Most work on the use of historical strike data (a) presupposes that _Batson_ law permits prior strikes as relevant evidence of bias in the current trial; presupposes that any statistical approach therein cannot automatically determine _Batson_ violations alone; and (c) adjusts in some way for incompatibility between historical strike data and strikes from the current trial. As an example, we now point in the paper's Introduction to @Gastwirth2014. Our approach has a distinctive benefit of making incompatibility assumptions transparent via choosing a fixed value for the power prior's $\alpha$.

_Page 4, line 5: The authors refer to gender but then provide sex options (male/female)._

Both the model and the prototype software encode "gender" as binary (male or female) simply because the underlying strike data in the app is based on standard juror questionnaires that provide "male" and "female" as the only mutually-exclusive options.

_Page 4, line 36: Use `\left(\right)` to ensure appropriately sized parentheses where necessary._

Thank you for spotting this.  The paper has been revised accordingly.

_Page 5, equation 6: The left-hand side-of the equation is not correct._

Thank you for spotting this.  The equation has been revised accordingly.

_Page 6, line 43: The idea behind $\alpha$ is concerning to me. The idea here is that if there is previous problematic strike data, and we choose large $\alpha$, then we’ll detect problematic strikes at higher rates. This type of modeling is helpful when historical data are informative to current random behavior, but in this case it goes against the usual ‘innocent until proven guilty,’ and, in fact, bias can be detected at rates close to 100% when there is no bias currently! That is, in other judicial contexts (and likely this one), this could be highly problematic. Simultaneously, large choices of $\alpha$ can lead to no detection of bias based on good behavior or lack of detection earlier. Note that detection rates are as low as 40% when current bias is set to 3! To me, noting I’m not a law expert, it seems a non-informative prior would be best here. While prior behavior could be used to demonstrate a pattern (guilt), I’m not convinced it’s appropriate for detection. If that is not the case, it should be thoroughly explained to the reader. Perhaps the citation reference by Reviewer 2 can help the authors – It’s possible that only considering small values of $\alpha$ would be reasonable. The simulation results for alpha = 0.1 are less objectionable._

It is true, based on our simulations, that bias can be erroneously detected (i.e. even if $b_curr$ = 0), if there are many past trials (e.g., n = 3) and in the bias in those trials exhibits strong incompatibility. On the other hand, given high compatibility, low values of $\alpha$ increase Type I error.  This is why we recommend using different values of $\alpha$ to check for sensitivity. See Item 1(2) above. 

The use of the word "guilty" in this comment implies that part of the concern here may turn on an error about the applicable standard of proof.  _Batson_ violations must be proven under the "preponderance of the evidence" standard (more likely than not), _not_ the more demanding  "reasonable doubt" standard that applies when a prosecutor aims to prove a defendant "guilty" of committing the elements of a criminal offense.

_Page 6, line 50: The description of bhist is not clear; it is defined twice with different values._

We revised the paper to clarify this description. 

_Page 9, line 15: The discussion about selecting credible interval percentages is confusing. Are you saying that larger credible interval percentages lead to less accurate bias detection?_

We revised to remove such confusion. This includes changing the software app to use 95% credible intervals. On the choice of credible intervals, see Item 1(5) above. The revision does not necessarily fix the type I error, because this coverage probability refers to the true bias parameter (that is fixed in our simulations), whereas we are interested in "bias detection", i.e. whether the credible interval includes 0 or not. The paper also has a sensitivity analysis for the selection of the nominal credible interval level.

_Page 17, line 46: The real data analyses side-step the use of prior data and don’t represent the analyses or simulation prior. One would be curious whether the approach with no prior works for detecting bias (e.g., simulations results without prior data). I think I would find this type of model more compelling._

This comment is not clear to us. The real data analyses are based on real historical strike data from real criminal cases in the federal district court in Connecticut. On how we collected this data, see the paper's Appendix. If they wish, users of the app can run the model excluding any historical data (as this reviewer suggested) by not selecting the name of a prosecutor or defense attorney. Like our model, the app uses the $N(0,2)$ prior for the bias parameter. Similarly, the app user can alter the weight of the historical strike data (if selected) by adjusting $\alpha$ (the power prior parameter). Thus, the app user can: (1) choose not to use historical data, (2) use historical data with equal weight as the current trial (alpha = 1), or (3) down-weight the historical trial data (alpha = 0.5 or alpha = 0.2). See also Item 1(6).

_Page 18, line 45: If there is benefit to the user for finding bias, the ability to select $\alpha$ is akin to p-hacking._

We respectfully disagree with the analogy to "p-hacking". The phrase "p-hacking" typically refers to the practice of using different statistical models on the same data in search of one or more associations that qualify as "statistically significant" (p < .05), and then reporting only the analysis with the model(s) that produce those results. In contrast, in our approach, the statistical model has to qualify as a valid model of the strike procedure of the court of interest. That strike procedure must be known and modeled before any strike data is analyzed. More generally, our approach thwarts the strategic selection of $\alpha$ values by making the choice of $\alpha$ transparent. See Item 1(4) and section 3.2 of the revised Discussion.
 
_Something unmentioned that should be addressed is confounders. While technological and statistical tools can be helpful to the judicial system, and even decrease instances of discrimination and curb disparities, there are many unintended consequences of such systems. For example, race and class are intertwined in the United States and there is no discussion (at least) about incorporating covariates (e.g., age, education, class, etc.)_`

On confounders, we expressly highlight (in the Introduction) that variation in trial-level characteristics (e.g. defendant race, charge severity) in the historical data can generate incompatibility, which in turn can affect bias detection. However, with respect to juror-level characteristics, the comment does not apply. Suppose a prosecutor strikes three Black prospective jurors on the belief that Black people are generally less educated and therefore more likely to favor defendants. This is illegal under _Batson_, regardless of the actual education levels of the three jurors struck. 

On unintended consequences, we found this comment unclear. The paper aims to show the validity of our Bayesian approach for incorporating historical strike data for estimating strike bias in the context of _Batson_ challenges. We do not aim to identify any and all consequences of a full-scale implementation of this approach, intended or unintended. Such implementation entails a series of operational choices that are well beyond the paper's scope. 

# Comments of Reviewer 1

_In Section 2.3, the paragraph in line 47 on p. 6 . . . I am confused about which set of values was used in the simulation for bhist._

Thank you for catching this.  We have revised the paper to clarify the values used in the simulation for $b_{hist}$.

_About the results presented in Figure 1 of Section 2.3, if we **ignore the assumption of compatibility** between historical and current data, in the cases of $\alpha$ = 0.5 and $\alpha$ = 1, does the bottom right corner in each panel indicate that the bias detection rate can be as high as 100% even when the true current bias is 0? If so, the Type I error seems to be very high in some cases. Similarly, the power (bias detection rate) seems low at the top left corner in each panel where bias is low in historical data but high in current data._

We agree that, given high incompatibility, (1) the bias "detection" rate can be as high as 100% when current bias is low ($b_curr$); and (2) bias detection can be low where current bias is high but bias in historical trials is low. The tradeoff is that, given high compatibility, historical strike data increases power that is otherwise low due the limited number of strikes per trial. The advantage of using the power prior: We can adjust the weight of the historical strike data (and therefore type I error) by adjusting $\alpha$.

_Regarding the Shiny App in Section 2.5, it would be helpful to include some details about the Metropolis-Hastings scheme. For instance, is a normal distribution used as the proposal distribution for b? What is the length of the Markov chain? Is any thinning performed on the posterior draws? Is any convergence criterion (e.g., the Gelman-Rubin statistic) checked? What is the computation time?_

This information has been added to the text (sec. 2.5, p. 18).

<!-- A normal distribution was assumed for the distribution of b. The length of the Markov chain was 110000 and the first 10000 iterations were dropped as burn-in. No thinning was performed, as the correlation was weak and the convergence occurred rapidly. Convergence was checked from both traceplots and the Gelman-Rubin statistic. Computation times varies from 0.76 to 1.2 seconds, depending in the amount of data used (current data only _versus_ current and historical data).--> 

_In Section 3, the authors state that "the anticipated degree of incompatibility should influence which value of $\alpha$ to select." As a follow-up of comment 2, if the assumption of compatibility is correct, setting $\alpha$ = 1 does lead to better power without sacrificing the Type I error. However, based on results in Figures 1 to 3, if this assumption is wrong, choosing a large $\alpha$ leads to either low power (bias detection rate) or large Type I error, depending on the bias in historical data. In the absence of such knowledge of compatibility, how does one choose the value of $\alpha$? In addition, to get around this issue, is it reasonable to treat $\alpha$ itself random and introduce a hyper prior?_

See Items 1(2) and 1(4) above and section 3.1. of the revised Discussion.

_Some of the Figure and table captions (e.g. Figures 1-5 and Table 1) can benefit from being more descriptive._

We have revised accordingly.

_A couple of notation issues: . . . ._

Thank you for pointing out these issues. We have revised Equations (3) and (6) accordingly.

# Comments of Reviewer 2

_The method is first illustrated in a simulation study, with a range of values of $\alpha$. This is fine for an audience of statisticians and attorneys specializing in jury discrimination but is somewhat overwhelming for a general reader. It would be easier for the reader if one data set, perhaps from the actual data from Connecticut was used and the reader could see how the data from each additional previous trial impacts the posterior. Perhaps only a few values of $\alpha$, rather than 11 and two val[u]es of the bias parameter b, could be used._

We appreciate this suggestion but we are worried that this would unduly increase the paper's length, particular given that absence of a closed form solution for the posterior distribution even in a toy example. Because of this concern, we have already moved other parts of the paper to the (online) Appendix. We intend the reader to gain understanding by following along using our prototype software application.

_From the likelihood on page 4, it is apparent that the method requires data on the consecutive strike patterns, not only in the current case, but in previous ones. How often is this data currently available? If it isn’t readily available now, how likely are states and locales to implement a system of systematically reporting and preserving the required data (especially in the areas where one expects minorities to encounter bias in jury membership)?_

Our statistical approach's validity turns neither on how many courts make available the kind of data it requires nor on proving how likely courts will start collecting such data. Indeed, as section 3.2 (paragraphs 1 and 2) of the paper explains, we hope our statistical approach can inform data collection efforts going forward both by courts and courtroom observers exercising their right at attend jury selection in criminal court proceedings.

_In Section 2.3, it is noted that in the simulations, it is assumed that the defense attorney has no bias. If one has been called for jury service in a city with a large minority population and been in a jury room, this is highly implausible. The reviewer once was sitting next to a minority woman and we saw---A minority leave the jury room, then a white, then another minority, then another white etc. At the end both of us noticed the “pattern”. Because one is analyzing a changing pool of possible challenges each time a peremptory challenge is made, when the defense attorney (assuming a minority defendant—the usual situation where Batson issues arise) removes a white, the minority proportion of the pool of possible individuals to be removed increases. This could make it more difficult to detect a biased prosecutor. On the other hand, if the method detects prosecutorial bias when the defense challenges appear biased too, this should strengthen the evidence of prosecutorial bias. This issue deserves some quantitative investigation, so judges can feel confident when using the results._

We assumed no strike bias for the defense attorney only to simplify exposition of the simulation results. Our model of the strike procedure (Equation 1) assumes that the probability of prosecutor strike bias is independent of the probability of defense attorney strike bias. And that model explicitly accounts for the racial composition of the pool of prospective jurors at each strike opportunity. In any case, we re-ran our simulation and found that, as expected, changing the level of defense attorney bias does not alter the bias detection rates for prosecutors (Table 1).

```{r inpts, echo=FALSE}

df <- data.frame( `Bias Pros` = c(1.5, 1.5, 1.5, 3.0, 3.0, 3.0, 1.5, 1.5, 1.5),
                  `Bias Def`= c(1.5, 1.5, 1.5, 3.0, 3.0, 3.0, 0.0, 0.0, 0.0),
                  `alpha`= c(0.1, 0.5, 1.0, 0.1, 0.5, 1.0, 0.1, 0.5, 1.0),
                  `Bias Det. Pros (95% CI)`= c(0.80, 0.99, 0.99, 0.99, 1.00, 1.00, 0.87, 1.00, 1.00))

knitr::kable(df, caption ="Simulation results showing effect of adjusting bias in defense on bias detection for the prosecutor. Changing the level of bias in defense does not alter bias detection rate for prosecutor.", format = "latex", booktabs = TRUE, align = "c")

```


_Page 5, equation 5: The left-hand side-of the equation is not correct._

We have revised the paper to indicate that it is a posterior distribution. All formulas have now been updated and checked. We now specify the vector notation for $\delta_0$ and $\delta_1$.

_Page 9, line 15: The discussion about selecting credible interval percentages is confusing. Are you saying that larger credible interval percentages lead to less accurate bias detection?_

See Item 1(5) above.

_The paper and software focus on peremptory challenges made by the same lawyer in previous cases. As noted in the Miller-El cases, the training manual used by prosecutors recommended the order in which groups, e.g. African-Americans, women etc. should be challenged. Therefore, data from prosecutors from the same DA’s office or defense firm in similar cases (drug dealing, robbery etc.), should be quite informative. Should one simply add them to the list of prior cases, or make a down-weighting adjustment in the Bayesian analysis?_

Thank you for this comment!  The paper now addresses this issue directly in the Discussion, section 3.1, paragraph 4.

_The paper uses 90% and 95% credible intervals on p. 7, 95% in Tables 2 and 3 and 80% on p. 18 and Fig. 7. From the various contexts, the reviewer got the impression that the author(s) were recommending that 0 not in the 95% credible interval should help the defendant establish their Batson claim, while 0 in an 80% credible interval indicates no bias and would help the prosecutor (in a typical Batson case) rebut a Batson challenge. Is this correct? In any case, the author(s) should indicate which credible intervals should be used at various stages of the proceedings. If possible, the standards for deciding a Batson claim used by judges in a jurisdiction should be the same. This is especially true in the present context where the prior data considered in cases in the same jurisdiction is likely to have a substantial overlap._

No, this is not correct. We have revised the paper and the software application to avoid such confusion. See Item 1(5) above. We agree that the choice of credible interval should be consistent within different stages of the _Batson_ challenge and across judges deciding _Batson_ challenges within the same jurisdiction.

_Gastwirth (2005) . . . should be noted. Also, if one had data from an actual case, one could show the additional insight one obtains by incorporating data from previous cases._

Thank you for this reference. The paper now cites to the model in Gastwirth (2005). On "data from an actual case," section 2.5 of the paper (our discussion about the prototype software application) expressly relies on actual strike data from a sample of criminal cases.

_The choice of $\alpha$ should be discussed and recommendations made. Otherwise, both parties can explore which value is most helpful. Also, how can one ensure that the prior data doesn’t overwhelm the data from the current case, without choosing a ridiculously low value? The authors say something about this on page 6 but more guidance to potential users and courts would be helpful._

On the choice of $\alpha$, see Items 1(2)-1(4) above and section 3.1 of the revised paper. At most, the historical strike data and the current strike data have _equal_ weight (when $\alpha$ = 1).

_The scenario depicted in Table 8 and the related text indicates that both the prosecutor and defense removed minority jurors –both 95% credible intervals do not contain 0. This seems unrealistic. More common would be the prosecutor removed more minorities so their 95% credible interval would not contain 0 while the credible interval for the defense removing non-minority potential jurors would be shifted but still might contain 0 (Of course, in some cases when minorities form a substantial portion, e.g. 40% or so of the venire, the data may indicate both sides were probably biased._

Even if it is "[m]ore common" for prosecutors to strike racial minorities, we intend the hypothetical data in Table 8, however "unrealistic", to underscore that, as  _Batson_ law requires, our approach makes no different assumptions for prosecutors than for defense attorneys when estimating attorney strike bias.

_At the bottom of p. 17, you don’t really need names for the attorneys, A and B would suffice._

We believe the names make it easier to read and follow along while using the app. The names are fake, courtesy of the `charlatan` package. See also Item 1(6).

# References
