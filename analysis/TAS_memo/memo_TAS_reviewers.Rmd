---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: false
    latex_engine: pdflatex
    template: "svm-latex-memo.tex"
    number_sections: true
fontfamily: mathpazo
fontsize: 11pt
geometry: margin=1in
header-includes:
   - \linespread{1.05}

from: Authors
to: Editor, The American Statistican
subject: Reply to Reviewer Comments
date: "`r format(Sys.Date(), '%B %d, %Y')`"
memorandum: true
graphics: false
width: .3
---

This memorandum replies to comments provided by the TAS associate editor and two reviewers.  It indicates how we have revised the paper accordingly, as well as our understanding of the issues raised by those comments.

# Comments of TAS Associate Editor

`[T]he article doesn’t compellingly make the case that the methodology is more useful than simply tracking those who have been reprimanded in the past or that it’s ethical/reasonable to actually employ in the courtroom. Specifically, the way that prior information is encoded, the logic behind a detection appears to be circular in that if someone has been reprimanded in the past they should be reprimanded in the future (in some cases for no reason). This issue hasn’t been discussed to a level that makes the utility of such a model clear.`

We have three replies to this concern.

First, as page X, lines Y of the paper indicate, the law of _Batson_ and parallel State law has consistently permitted judges to infer illegal strike bias based in part on an attorney's use of strikes in past cases. In reviewing the kinds of evidence that can prove a _Batson_ violation by a prosecutor, the U.S. Supreme Court included, as examples of types of evidence, the "relevant history of the State’s peremptory strikes in past cases." Flowers v. Mississippi, 139 S. Ct. 2228, pin (2019). More recent State law variants on _Batson_ are also explicit about this. For example, Washington State law provides that in deciding whether "an objective observer could view race or ethnicity as a factor in the use of the peremptory challenge," the circumstances the court "should" consider include "whether the party has used peremptory challenges disproportionately against a given race or ethnicity, in the present case _or in past cases_." Wash. General Rule 37(g)(v) (emphasis added). California law similarly identifies, as a circumstance a court may consider, whether the striking "has used peremptory challenges disproportionately against a given race, ethnicity, gender, gender identity, sexual orientation, national origin, or religious affiliation, or perceived membership in any of those groups, in the present case _or in past cases_." Calif. Code of Civil Procedure 231.7(d)(3)(G) (emphasis added).

We hope this removes the worry that it is not "ethical/reasonable" to infer present strike bias from strike behavior in past cases. 

Second, as the phrase "should be reprimanded in the future (in some cases for no reason)" implies, the associate editor seems worried that our approach will cause judges to infer that illegal strike bias is more likely than not (i.e., find _Batson_ violations ) when in fact no strike bias exists. 

This is not the case, and we have revised the paper to make this more clear. 

First, we assume the word "reprimanded" refers to a court finding of a _Batson_ violation. Our approach, however, relies on an attorney's use of strikes of past cases, regardless of whether those strikes were ever the subject of a _Batson_ challenge. This is better than just "tracking those who have been reprimanded in the past" for _Batson_ violations, because the probability of a court that finds a _Batson_ violation depends on an attorney having raised a _Batson_ challenge in the first place. (Absent an attorney arguing _Batson_, a court will not inquire into whether illegal strike bias existed.) If illegal strike bias exists but the attorney erroneously perceives no credible basis for it, the attorney will tend not to argue _Batson_. In cases where there no illegal strike bias but the attorney erroneously perceives some credible basis for it, the attorney will tend to argue _Batson_ but may not persuade the trial judge that illegal strike bias was more likely than not. By relying on past strike data, not whether anyone argued _Batson_ in those past cases, we avoid any selection bias associated with relying only on past court findings of _Batson_ violations.

Second, _Batson_ procedure permits any type of admissible evidence of strike bias (for or against) -- not just the attorney's past strike data -- and requires the trial judge to weigh all such evidence to decide whether illegal strike bias exists. For this reason, our approach alone is not an all-purpose detector of _Batson_ violations, because our approach cannot incorporate the weight of all the other kinds of evidence that could be submitted to prove illegal strike bias in the present case. For example, _Batson_ procedure requires the striking attorney at a certain stage to justify the challenged strikes based on (non-discriminatory) reasons. If the attorney fails to produce such reasons, the trial judge must find a _Batson_ violation. If the attorney offers such reasons, the trial judge must assess evidence as to the credibility of those reasons. 

Our approach cannot adjust the posterior distribution of the bias parameter to account for the credibility of those reasons or other types of evidence (other than the attorney's pattern of strikes) of illegal strike bias. Rather, as the new Figure 1 depicts, our approach helps attorneys evaluate the inference of illegal bias in the present case based on historical strike data but _before_ accounting for other evidence relevant to illegal strike bias. For that task, our approach assigns at most equal weight to past strikes and their strikes in the present case (i.e., when $\alpha = 1$).

The possible incompatability of $b_hist$ and $b_curr$ is already and always an issue when using past strike data to infer bias. This is a reality faced by _any_ statistical technique for inferring strike bias using past strike data. 
Because the law of _Batson_ expressly permits inferring current strike bias based on past strikes in previous trials, the lawyers in _Batson_ challenges involving past strike data are already and always arguing about how much weight to assign to that past strike data. 

Our approach simply makes the weighting of historical strike data -- usually expressed with words -- more explicit and transparent by requiring a choice of a value for alpha. The more sensitive the inference of bias (zero falls outside the credible interval) is to the choice of $\alpha$, the more one should worry about incompatibility. In turn, the appropriateness of small vs. large values of $\alpha$ vary with the suspected degree of incompatability. In turn, attorneys can use that information, _combined with_ other evidence, to (1) decide whether to argue _Batson_, (2) to prove the _Batson_ prima facie case, and (3) to persuade the trial judge to find that illegal strike bias is more likely than not, given all the evidence, i.e., find a _Batson_ violation. 

We hope that these modifications clarify the placement of our model in the steps from a sequence of peremptory challenges to a _Batson_ challenge actually being brought, and upheld by the trial judge.

### Additional Comments

`1. Page 4, line 5: The authors refer to gender but then provide sex options (male/female).`

[comment]

`2. Page 4, line 36: Use \left(\right) to ensure appropriately sized parentheses where necessary.`

Thank you for this suggestion. The paper has been revised accordingly.

`3. Page 5, equation 6: The left-hand side-of the equation is not correct`

Thank you for spotting this.  The equation has been revised accordingly.

`4. Page 6, line 43: The idea behind $\alpha$ is concerning to me. The idea here is that if there is previous problematic strike data, and we choose large $\alpha$, then we’ll detect problematic strikes at higher rates. This type of modeling is helpful when historical data are informative to current random behavior, but in this case it goes against the usual ‘innocent until proven guilty,’ and, in fact, bias can be detected at rates close to 100% when there is no bias currently! That is, in other judicial contexts (and likely this one), this could be highly problematic. Simultaneously, large choices of $\alpha$ can lead to no detection of bias based on good behavior or lack of detection earlier. Note that detection rates are as low as 40% when current bias is set to 3! To me, noting I’m not a law expert, it seems a non-informative prior would be best here. While prior behavior could be used to demonstrate a pattern (guilt), I’m not convinced it’s appropriate for detection. If that is not the case, it should be thoroughly explained to the reader. Perhaps the citation reference by Reviewer 2 can help the authors – It’s possible that only considering small values of $\alpha$ would be reasonable. The simulation results for alpha = 0.1 are less objectionable.`



Finally, the reviewer's use of the word "guilty" requires two clarifying notes.  First, _Batson_ violations must be proven under the "preponderance of the evidence" standard (more likely than not), _not_ the more stringent "reasonable doubt" standard that applies when a prosecutor aims to prove a defendant "guilty" of committing the criminal offense charged. Second, as explained above, bias "detection" in the paper means only whether the credible interval for the bias parameter ($b_{curr}) excludes zero. Because _Batson_ procedure requires weighing any and all evidence of strike bias, not just past strike data, our approach cannot alone identify _Batson_ violations in cases involving other relevant evidence bias besides past strike data.

`5. Page 6, line 50: The description of bhist is not clear; it is defined twice with different values.`

The paper has been revised to clarify this description. 

`6. Page 9, line 15: The discussion about selecting credible interval percentages is confusing. Are you saying that larger credible interval percentages lead to less accurate bias detection?`

We have revised this discussion to remove such confusion.  The choice of the credible interval is a decision about how conservative one wishes to be about inferring bias from past strike data alone. We define bias "detection" narrowly to mean when $b = 0$ falls outside the chosen credible interval. Thus, the larger the credible interval, the greater the chance that zero falls within it. For the sake of clarity, we now focus on one credible interval (95%) in the paper and have placed simulation results based on other credible intervals in the Appendix. This does not necessarily fix the type I error, because this coverage probability refers to the true bias parameter (that is fixed in our simulations), and we are interested in "bias detection", i.e. whether the credible interval includes 0 or not. We also present sensitivity analysis for the selection of the nominal HPD level.

`7. Page 17, line 46: The real data analyses side-step the use of prior data and don’t represent the analyses or simulation prior. One would be curious whether the approach with no prior works for detecting bias (e.g., simulations results without prior data). I think I would find this type of model more compelling.`

This comment is not clear to us. The real data analyses are based on real _historical_ trial data from the federal district court in Connecticut (see lines x to y). Users of the app have the option to run the model excluding any historical trial data (as suggested by the comment) by not selecting the name of a prosecutor or defense attorney. In this case, the normal(0,2) prior is used for the bias parameter. Similarly, the app gives the user the option of altering the weight the historical strike data (if selected) by adjusting alpha (power prior parameter). Thus the user can: 1) choose not to use historical data, 2) use historical data with equal weight as the current trial (alpha = 1), or 3) down-weight the historical trial data.

`8. Page 18, line 45: If there is benefit to the user for finding bias, the ability to select $\alpha$ is akin to p-hacking.`

We respectfully disagree with the analogy to "p-hacking", for two reasons.  First, the phrase "p-hacking" typically refers to the practice of using different statistical models on the same data in search of one or more associations that qualify as "statistically significant" (p < .05), and then reporting only the analysis with the model(s) that produce those results. In contrast, in our approach, the statistical model necessarily precedes any analysis, because it has to qualify as a valid model of the strike _procedure_ of the court of interest, which can and must be known before any data is collected or analyzed. 

Second, in the adversarial setting of the courtroom, our approach actually thwarts the strategic selection of $\alpha$ values, because it makes the choice of $\alpha$ transparent.  This is why we stress in the paper (p. X, lines Y) that, where there is suspected incompatibility between $b_{hist}$ and $b_{curr}$, the proper approach is try multiple values of $\alpha$ and determine how sensitive the inference of bias is to the chosen $\alpha$ value.  Because the choice of how much to weigh the historical strike data is transparent, every lawyer knows that any strategic selection of $\alpha$ will be met with scrutiny from the lawyers on the other side. Absent this transparency, lawyers already still argue for how much past strike behavior ought to weigh when inferring or dispute illegal strike bias, except that they do so implicitly and with the opacity of words.   
 
`9. Something unmentioned that should be addressed is confounders. While technological and statistical tools can be helpful to the judicial system, and even decrease instances of discrimination and curb disparities, there are many unintended consequences of such systems. For example, race and class are intertwined in the United States and there is no discussion (at least) about incorporating covariates (e.g., age, education, class, etc.)`

[INSERT] Batson challenge doesn't care if you strike a cognizable class because you assume they have particular confounders (e.g., low income, etc). [editme] We do explicitly highlight that trial-level characteristics and impact compatibility between current and historical trials. We do advocate for reassessment of the way (and what) data are collected from individual trials. 

# Reviewer 1

>1. In Section 2.3, the paragraph in line 47 on p. 6 . . . I am confused about which set of values was used in the simulation for bhist.

Thank you for catching this.  We have revised the paper (p. X, lines Y) to clarify the values used in the simulation for $b_{hist}$.

>2. About the results presented in Figure 1 of Section 2.3, if we ignore the assumption of compatibility between historical and current data, in the cases of $\alpha$ = 0:5 and $\alpha$ = 1, does the bottom right corner in each panel indicate that the bias detection rate can be as high as 100% even when the true current bias is 0? If so, the Type I error seems to be very high in some cases. Similarly, the power (bias detection rate) seems low at the top left corner in each panel where bias is low in historical data but high in current data.

We agree that the bias "detection" rate (whether the credible interval excludes zero) can be as high as 100% when true bias is zero. We also agree that the power to detect bias is low when there is low bias in historical trials. These situations can occur because of high incompatibility.  Similarly, the low power situations occurs due the limited number of strikes, which is why _compatible_ historical strike data can increase this power. 

This is an advantage of using the power prior: We can adjust the weight of the historical strike data (and therefore type I error) by adjusting alpha. In reality, because $b_{curr}$ and $b_{hist}$ are not directly observable, one will need to rely on background knowledge or assumptions about the degree of incompatibility to decide which value of alpha to choose. If they bring a _Batson_ challenge, they must also disclose and justify their choice of alpha and the assumptions on which that choice is based.

>3. Regarding the Shiny App in Section 2.5, it would be helpful to include some details about the Metropolis-Hastings scheme. For instance, is a normal distribution used as the proposal distribution for b? What is the length of the Markov chain? Is any thinning performed on the posterior draws? Is any convergence criterion (e.g., the Gelman-Rubin statistic) checked? What is the computation time?

[INSERT] Will get Eric to check this (still no response from Xiaomeng)

>4. In Section 3, the authors state that "the anticipated degree of incompatibility should influence which value of $\alpha$ to select." As a follow-up of comment 2, if the assumption of compatibility is correct, setting $\alpha$ = 1 does lead to better power without sacrificing the Type I error. However, based on results in Figures 1 to 3, if this assumption is wrong, choosing a large $\alpha$ leads to either low power (bias detection rate) or large Type I error, depending on the bias in historical data. In the absence of such knowledge of compatibility, how does one choose the value of $\alpha$? In addition, to get around this issue, is it reasonable to treat $\alpha$ itself random and introduce a hyper prior?

As indicated in the response to comment 2 above, one chooses a value of $alpha$ based on background beliefs on plausible degree of incompatibility, and then evaluate how sensitive bias detection (whether zero falls outside the credible interval) is to the choice of alpha value. Absent some very strong assumptions, there is no generally justifiable hyper prior for $\alpha$, because the proper weight to place on historical data depends on whether, for that historical data, there is any missing data that is missing not at random and whether an inference of bias based on that historical strike data is complicated by plausible confounding or mediating trial-level variables (e.g., defendant race, criminal charge severity). The plausible reasons for non-random missingness and confounding, however, likely vary considerably depending on strike procedure, courtroom practices, prosecutor charging practices, among other things.

>5. Some of the Figure and table captions (e.g. Figures 1-5 and Table 1) can benefit from being more descriptive.

[INSERT] 

> 6. A couple of notation issues: . . . .

Thank you for pointing out these issues. We have revised Equations (3) and (6) accordingly.

# Reviewer 2

>1. The method is first illustrated in a simulation study, with a range of values of $\alpha$. This is fine for an audience of statisticians and attorneys specializing in jury discrimination but is somewhat overwhelming for a general reader. It would be easier for the reader if one data set, perhaps from the actual data from Connecticut was used and the reader could see how the data from each additional previous trial impacts the posterior. Perhaps only a few values of $\alpha$, rather than 11 and two val[u]es of the bias parameter b, could be used.

* Start with CT data as example. Take screenshots from the app part. 

> 2. From the likelihood on page 4, it is apparent that the method requires data on the consecutive strike patterns, not only in the current case, but in previous ones. How often is this data currently available? If it isn’t readily available now, how likely are states and locales to implement a system of systematically reporting and preserving the required data (especially in the areas where one expects minorities to encounter bias in jury membership)?

In the Discussion, we explained that data on consecutive strike patterns is hard to collect, simply because of the challenges of collecting any historical strike data, regardless of strike procedure. We have revised that Discussion to add that, since the 1980s, any person has a First Amendment right to attend jury selection in criminal court proceedings, and criminal defendants have a Sixth Amendment right to public court proceedings during jury selection. These legal rights, in turn, enable data collection by court observers, regardless of State and local recordkeeping practices. 

Finally, the current availability of the data required by the model seems orthogonal to the merits of our statistical approach of using such data to infer bias. [MORE].

* Repeat comment about accessibility

`3. In Section 2.3, it is noted that in the simulations, it is assumed that the defense attorney has no bias. If one has been called for jury service in a city with a large minority population and been in a jury room, this is highly implausible. The reviewer once was sitting next to a minority woman and we saw---A minority leave the jury room, then a white, then another minority, then another white etc. At the end both of us noticed the “pattern”. Because one is analyzing a changing pool of possible challenges each time a peremptory challenge is made, when the defense attorney (assuming a minority defendant—the usual situation where Batson issues arise) removes a white, the minority proportion of the pool of possible individuals to be removed increases. This could make it more difficult to detect a biased prosecutor. On the other hand, if the method detects prosecutorial bias when the defense challenges appear biased too, this should strengthen the evidence of prosecutorial bias. This issue deserves some quantitative investigation, so judges can feel confident when using the results.`

* [TEM&EB: Check this to see if adding bias to the defense alters bias detection for the prosecutors. ][see table below]










Data for past trials is not readily available. But, since the 80s, any person can enter a courtroom and observe the jury selection process, so this data could be collected for present and future cases. 


* [add paragraph to address this]


Further, in ‘Other Comments’ 3-4, they describe a rather important realities about the application that are not accounted for in the model. While the problem is certainly interesting, the simulations demonstrate some highly problematic results.
Knowledgeable reviewers and I still have several large unanswered questions about how the methodology can be enacted appropriately. I provide my comments below.



3. Page 5, equation 5: The left-hand side-of the equation is not correct
* [EB: fix in formula] - This is now fixed to reflect that it is a posterior distribution. We note that all formulas have now been updated and checked. 

We now specify the vector notation for $\delta_0$ and $\delta_1$.

Mention that in the app, it is easy to check the influence of alpha by re-running the model. Make a clearer case that level of alpha should be selected in cases of high incompatibility. Law allows you to rely on the prosecutor's past behavior. The ability to modify alpha, allows you to decide whether inference based on past behavior is warranted.
[TEM & EB]

6. Page 9, line 15: The discussion about selecting credible interval percentages is confusing. Are you  saying that larger credible interval percentages lead to less accurate bias detection?

7. Page 17, line 46: The real data analyses side-step the use of prior data and don’t represent the analyses or simulation prior. One would be curious whether the approach with no prior works for detecting bias (e.g., simulations results without prior data). I think I would find this type of model more compelling.

The real data analysis, as the default option, does not borrow historical data (i.e. $\alpha=0$). In this case the default prior of $N(0,2)$ is used for the bias parameter (i.e. b). This is reflected when no attorney is selected from the pull-down menu. If an attorney is selected from the pull-down menu, then prior information is used for that attorney. The paper has been updated to reflect this point in the same section.

Main Comments:


`4. The paper and software focus on peremptory challenges made by the same lawyer in previous cases. As noted in the Miller-El cases, the training manual used by prosecutors recommended the order in which groups, e.g. African-Americans, women etc. should be challenged. Therefore, data from prosecutors from the same DA’s office or defense firm in similar cases (drug dealing, robbery etc.), should be quite informative. Should one simply add them to the list of prior cases, or make a down-weighting adjustment in the Bayesian analysis?`

* Could use same idea for individual lawyers for offices, by aggregating by firms/office/etc, and remove duplicate strikes/trials.

[SP: add pararaph/sentence about this]

`5. The paper uses 90% and 95% credible intervals on p. 7, 95% in Tables 2 and 3 and 80% on p. 18 and Fig. 7. From the various contexts, the reviewer got the impression that the author(s) were recommending that 0 not in the 95% credible interval should help the defendant establish their Batson claim, while 0 in an 80% credible interval indicates no bias and would help the prosecutor (in a typical Batson case) rebut a Batson challenge. Is this correct? In any case, the author(s) should indicate which credible intervals should be used at various stages of the proceedings. If possible, the standards for deciding a Batson claim used by judges in a jurisdiction should be the same. This is especially true in the present context where the prior data considered in cases in the same jurisdiction is likely to have a substantial overlap.`

* This is not correct. Clarify by focussing on single interval.

`Gastwirth (2005) in Law, Prob. Risk, p. 179-185, . . . had proposed that approach when potential jurors were removed one at a time in an alternating sequence. Checking that reference, the reviewer saw that at the end of the 2005
paper, the use of data from prior cases would be helpful and recommended that procedures, such as the one proposed here. This connection should be noted. Also, if one had data from an actual case, one could show the additional insight one obtains by incorporating data from previous cases.`

Thank you for this reference.  The paper now cites to the model in Gastwirth (2005). We also agree about the "additional insight" from "data from an actual case."  The discussion about the prototype software application expressly relies on actual strike data from a sample of trials from the federal district court of Connecticut. [MORE].

`The choice of $\alpha$ should be discussed and recommendations made. Otherwise, both parties can explore which value is most helpful. Also, how can one ensure that the prior data doesn’t overwhelm the data from the current case, without choosing a ridiculously low value? The authors say something about this on page 6 but more guidance to potential users and courts would be helpful.`

* Repeat section about alpha/metal detector argument. 

`The scenario depicted in Table 8 and the related text indicates that both the prosecutor and defense removed minority jurors –both 95% credible intervals do not contain 0. This seems unrealistic. More common would be the prosecutor removed more minorities so their 95% credible interval would not contain 0 while the credible interval for the defense removing non-minority potential jurors would be shifted but still might contain 0 (Of course, in some cases when minorities form a substantial portion, e.g. 40% or so of the venire, the data may indicate both sides were probably biased.`

Table 8 = hypothetical data. This is just one scenario. Nothing about the tool makes any assumptions about what defense and prosecutors do/tend to do when it comes to cognizable/non-cognizable jurors. The burden of proof lies with the person bringing the challenge

> At the bottom of p. 17, you don’t really need names for the attorneys, A and B would suffice.

Fair. Although, these are fake names.

Update online appendix
* Move figures (80% and 90% CI)
   * 80 and 90 CI
   * Fig 4, 6, 8, 9,
* Move app screenshots and majority of text about app
* Add sensitivity for defense bias (new)
