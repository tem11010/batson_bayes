---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: false
    latex_engine: pdflatex
    template: "svm-latex-memo.tex"
    number_sections: true
fontfamily: mathpazo
fontsize: 11pt
geometry: margin=1in
header-includes:
   - \linespread{1.05}

from: Authors
to: Editor, The American Statistican
subject: Reply to Reviewer Comments
date: "`r format(Sys.Date(), '%B %d, %Y')`"
memorandum: true
graphics: false
width: .3
---

This memorandum replies to comments provided by the TAS associate editor and two reviewers. We believe the revised paper is better as a result.

# Key Issues

Here, we address five key issues raised by multiple reviewer comments.

1. Our statistical approach is not an all-purpose detector of _Batson_ violations. No statistical approach can do this, because _Batson_ procedure permits any type of admissible evidence of illegal strike bias -- not just the attorney's past strike data -- and requires the trial judge to weigh all such evidence to decide whether illegal strike bias exists. For example, _Batson_ procedure requires the striking attorney at a certain stage to justify the challenged strikes based on (non-discriminatory) reasons. If the attorney fails to produce such reasons, the trial judge must find a _Batson_ violation. If the attorney offers such reasons, the trial judge must assess evidence as to the credibility of those reasons. Our statistical approach cannot adjust the posterior distribution of the bias parameter to account for the credibility of those reasons or other types of evidence (other than the attorney's pattern of strikes) of illegal strike bias. Rather, our approach helps attorneys evaluate the inference of illegal bias in the present case based on historical strike data but _before_ accounting for other evidence relevant to illegal strike bias. The phrase "bias detection" in the paper means only that the credible interval for the bias parameter ($b_{curr}$) excludes zero, given the strike data and the model. For that "detection" task, our approach lets one assign at most equal weight to past strikes and their strikes in the present case ($\alpha = 1$). In turn, attorneys can use the resulting bias estimate, _combined with_ other evidence, to (1) decide whether to argue _Batson_, (2) to prove the _Batson_ prima facie case, and (3) to persuade the trial judge to find  a _Batson_ violation (i.e., illegal strike bias is more likely than not, given all the evidence). We have revised the Introduction to emphasize all this to the reader at the outset.

2. Possible incompatibility of $b_{hist}$ and $b_{curr}$ is always an issue when using past strike data to infer strike bias in the current trial. This is the case for _any_ statistical approach that uses past strike data to infer strike bias in the current trial. And in _Batson_ challenges involving past strike data, the lawyers are already and always arguing about how much weight to assign to that past strike data, albeit with words. By using the power prior, our approach simply makes the weighting of historical strike data more explicit and transparent as a choice of a value for $\alpha$. The more sensitive bias "detection" (i.e. whether the posterior estimate of $b_{curr}$ excludes zero) is to the choice of $\alpha$, the more one should worry about incompatibility. Thus, small vs. large values of $\alpha$ should vary with the suspected degree of incompatability. In reality, because $b_{curr}$ and $b_{hist}$ are not directly observable, one will need to rely on background knowledge or assumptions about the degree of incompatibility to decide which value of $\alpha$ to choose. In a _Batson_ challenge, the lawyers must disclose and justify their choice of $\alpha$ accordingly. We have revised the paper to emphasize this.

3. In the adversarial setting of the courtroom, our approach thwarts the strategic selection of $\alpha$ values, because it makes the choice of $\alpha$ transparent, and thus the choice of how much to weigh the historical strike data.  As a result, every lawyer will know that any strategic selection of $\alpha$ will be met with scrutiny from the lawyers on the other side. Absent this transparency, lawyers will continue to argue strategically about how much to weigh historical strike data, except with the opacity of words.  We have revised the paper to add this to the Discussion section.

4. Absent some very strong assumptions, there is no generally justifiable hyper prior for $\alpha$ in this context of strike bias. This is because the proper weight to place on historical data depends on whether, for that historical data, there is any missing data that is missing not at random and whether inferring bias from that historical strike data is complicated by plausible confounding due to trial-level variables (e.g., defendant race, criminal charge severity). The plausible reasons for non-random missingness and such confounding, however, likely vary considerably depending on strike procedure, courtroom practices, prosecutor charging practices, among other things. As we now note in the Discussion section of the paper, measuring incompatibility and choosing $\alpha$ in the face of incompatibility is an active area of research with no clear solution.

5. Choosing a credible interval amounts to deciding how conservative one wishes to be about inferring bias from past strike data, because we define bias "detection" narrowly to mean when the posterior for the bias parameter falls outside the chosen credible interval. Thus, the larger the credible interval one chooses, the greater the chance that zero falls within it. For clarity, we now focus on one credible interval (95%) in the paper and have placed simulation results based on other credible intervals in the Appendix. 

In the following responses to particular comments, we will refer to the above items when appropriate.

# Comments of TAS Associate Editor

>[T]he article doesn’t compellingly make the case that the methodology is more useful than simply tracking those who have been reprimanded in the past or that it’s ethical/reasonable to actually employ in the courtroom. Specifically, the way that prior information is encoded, the logic behind a detection appears to be circular in that if someone has been reprimanded in the past they should be reprimanded in the future (in some cases for no reason). This issue hasn’t been discussed to a level that makes the utility of such a model clear.

We have three replies to this concern.

First, the law of _Batson_ and parallel State law has consistently permitted judges to infer illegal strike bias based in part on an attorney's use of strikes in past cases. In reviewing the kinds of evidence that can prove a _Batson_ violation by a prosecutor, the U.S. Supreme Court included, as examples of types of evidence, the "relevant history of the State’s peremptory strikes in past cases." _Flowers v. Mississippi_, 139 S. Ct. 2228, [PIN] (2019). State law variants on _Batson_ are also explicit about this. For example, Washington State law provides that in deciding whether "an objective observer could view race or ethnicity as a factor in the use of the peremptory challenge," the circumstances the court "should" consider include "whether the party has used peremptory challenges disproportionately against a given race or ethnicity, in the present case _or in past cases_." Wash. General Rule 37(g)(v) (emphasis added). California law similarly identifies, as a circumstance a court may consider, whether the striking party "has used peremptory challenges disproportionately against a given race . . . in the present case _or in past cases_." Calif. Code of Civil Procedure 231.7(d)(3)(G) (emphasis added).

We hope this removes the worry that it is not "ethical/reasonable" to infer present strike bias from strike behavior in past cases. 

Second, as the phrase "should be reprimanded in the future (in some cases for no reason)" implies, the apparent worry is that our approach will cause judges to infer, in any particular case, that illegal strike bias is more likely than not (i.e., find _Batson_ violations) when in fact it was less likely than not. This is not the case. Our approach does not and cannot automate findings of _Batson_ violations. See Item 1, above.

Third, we assume the word "reprimanded" refers to a court finding of a _Batson_ violation. Our approach, however, relies on an attorney's use of strikes of past cases, regardless of whether those strikes were ever the subject of a _Batson_ challenge. This is better than just "tracking those who have been reprimanded in the past" for _Batson_ violations, because the probability that a court finds a _Batson_ violation depends on an attorney having raised a _Batson_ challenge in the first place. (Absent an attorney arguing _Batson_, a court will not inquire into whether illegal strike bias existed.) If illegal strike bias exists but the attorney erroneously perceives no credible basis for it, the attorney will tend not to argue _Batson_. In cases where there no illegal strike bias but the attorney erroneously perceives some credible basis for it, the attorney will tend to argue _Batson_ but may not persuade the trial judge that illegal strike bias was more likely than not. By relying on past strike data, not whether anyone argued _Batson_ in those past cases, we avoid any bias associated with relying only on past court findings of _Batson_ violations.

>Page 4, line 5: The authors refer to gender but then provide sex options (male/female).`

Both the model and the prototype software encode "gender" as binary (male or female) simply because the underlying strike data in the app is based on standard juror questionaires that provide "male" and "female" as the only mutually-exclusive options.

>Page 4, line 36: Use `\left(\right)` to ensure appropriately sized parentheses where necessary.`

Thank you for spotting this.  The paper has been revised accordingly.

>Page 5, equation 6: The left-hand side-of the equation is not correct`

Thank you for spotting this.  The equation has been revised accordingly.

>Page 6, line 43: The idea behind $\alpha$ is concerning to me. The idea here is that if there is previous problematic strike data, and we choose large $\alpha$, then we’ll detect problematic strikes at higher rates. This type of modeling is helpful when historical data are informative to current random behavior, but in this case it goes against the usual ‘innocent until proven guilty,’ and, in fact, bias can be detected at rates close to 100% when there is no bias currently! That is, in other judicial contexts (and likely this one), this could be highly problematic. Simultaneously, large choices of $\alpha$ can lead to no detection of bias based on good behavior or lack of detection earlier. Note that detection rates are as low as 40% when current bias is set to 3! To me, noting I’m not a law expert, it seems a non-informative prior would be best here. While prior behavior could be used to demonstrate a pattern (guilt), I’m not convinced it’s appropriate for detection. If that is not the case, it should be thoroughly explained to the reader. Perhaps the citation reference by Reviewer 2 can help the authors – It’s possible that only considering small values of $\alpha$ would be reasonable. The simulation results for alpha = 0.1 are less objectionable.`

It is true, based on our simulations, that bias can be erroneously detected (i.e. even if $b_curr$ = 0), if there are many past trials (e.g., n = 3) and in the bias in those trials exhibits strong incompatibilty. On the other hand, given high compatibility, low values of $\alpha$ increase Type I error.  This is why we recommend using different values of $\alpha$ to check for sensitivity. See Item 2 above.

The use of the word "guilty" is error.  _Batson_ violations must be proven under the "preponderance of the evidence" standard (more likely than not), _not_ the more stringent "reasonable doubt" standard that applies when a prosecutor aims to prove a defendant "guilty" of committing the criminal offense charged. 

>Page 6, line 50: The description of bhist is not clear; it is defined twice with different values.`

We revised the paper to clarify this description. 

>Page 9, line 15: The discussion about selecting credible interval percentages is confusing. Are you saying that larger credible interval percentages lead to less accurate bias detection?`

We revised to remove such confusion. On the choice of credible intervals, see Item 5 above. The revision does not necessarily fix the type I error, because this coverage probability refers to the true bias parameter (that is fixed in our simulations), whereas we are interested in "bias detection", i.e. whether the credible interval includes 0 or not. The paper also has a sensitivity analysis for the selection of the nominal credible interval level.

>Page 17, line 46: The real data analyses side-step the use of prior data and don’t represent the analyses or simulation prior. One would be curious whether the approach with no prior works for detecting bias (e.g., simulations results without prior data). I think I would find this type of model more compelling.`

This comment is not clear to us. The real data analyses are based on real historical strike data from real criminal cases in the federal district court in Connecticut. If they wish, users of the app can run the model excluding any historical data (as this reviewer suggested) by not selecting the name of a prosecutor or defense attorney. Like our model, the app uses the $N(0,2)$ prior for the bias parameter. Similarly, the app user can alter the weight the historical strike data (if selected) by adjusting $\alpha$ (the power prior parameter). Thus, the app user can: (1) choose not to use historical data, (2) use historical data with equal weight as the current trial (alpha = 1), or (3) down-weight the historical trial data (alpha = 0.5 or alpha = 0.2).

>Page 18, line 45: If there is benefit to the user for finding bias, the ability to select $\alpha$ is akin to p-hacking.`

We respectfully disagree with the analogy to "p-hacking". The phrase "p-hacking" typically refers to the practice of using different statistical models on the same data in search of one or more associations that qualify as "statistically significant" (p < .05), and then reporting only the analysis with the model(s) that produce those results. In contrast, in our approach, the statistical model has to qualify as a valid model of the strike _procedure_ of the court of interest. That strike procedure can and must be known and modeled before any strike data is collected or analyzed. More generally, our approach thwarts the strategic selection of $\alpha$ values by making the choice of $\alpha$ transparent. See Item 4, above.
 
>Something unmentioned that should be addressed is confounders. While technological and statistical tools can be helpful to the judicial system, and even decrease instances of discrimination and curb disparities, there are many unintended consequences of such systems. For example, race and class are intertwined in the United States and there is no discussion (at least) about incorporating covariates (e.g., age, education, class, etc.)`

On confounders, we expressly highlight that variation in trial-level characteristics (e.g. defendant race, charge severity) in the historical data can generate incompatibility, which in turn can affect bias detection. However, with respect to juror-level characteristics, the comment does not apply. Suppose a prosecutor strikes three Black prospective jurors on the belief that Black people are generally less educated and therefore more likely to favor defendants. This is illegal under _Batson_, regardless of the actual education levels of the three jurors struck. [FIX].

On unintended consequences, we found this comment unclear. The paper aims to show the validity of our Bayesian approach for incorporating historical strike data for estimating strike bias in the context of _Batson_ challenges. We do not aim to identify any and all consequences of a full-scale implementation of this approach, intended or unintended. 

# Reviewer 1

>In Section 2.3, the paragraph in line 47 on p. 6 . . . I am confused about which set of values was used in the simulation for bhist.

Thank you for catching this.  We have revised the paper to clarify the values used in the simulation for $b_{hist}$.

>About the results presented in Figure 1 of Section 2.3, if we **ignore the assumption of compatibility** between historical and current data, in the cases of $\alpha$ = 0.5 and $\alpha$ = 1, does the bottom right corner in each panel indicate that the bias detection rate can be as high as 100% even when the true current bias is 0? If so, the Type I error seems to be very high in some cases. Similarly, the power (bias detection rate) seems low at the top left corner in each panel where bias is low in historical data but high in current data.

We agree that, given high incompatibility, (1) the bias "detection" rate can be as high as 100% when current bias is low ($b_curr$); and (2) bias detection can be low where current bias is high but bias in historical trials is low. The tradeoff is that, given compatibility, historical strike data increases power that is otherwise low due the limited number of strikes per trial. The advantage of using the power prior: We can adjust the weight of the historical strike data (and therefore type I error) by adjusting $alpha$.

>Regarding the Shiny App in Section 2.5, it would be helpful to include some details about the Metropolis-Hastings scheme. For instance, is a normal distribution used as the proposal distribution for b? What is the length of the Markov chain? Is any thinning performed on the posterior draws? Is any convergence criterion (e.g., the Gelman-Rubin statistic) checked? What is the computation time?

[INSERT] Will get Eric to check this (got a response from Xiaomeng - waiting for follow-up)

>In Section 3, the authors state that "the anticipated degree of incompatibility should influence which value of $\alpha$ to select." As a follow-up of comment 2, if the assumption of compatibility is correct, setting $\alpha$ = 1 does lead to better power without sacrificing the Type I error. However, based on results in Figures 1 to 3, if this assumption is wrong, choosing a large $\alpha$ leads to either low power (bias detection rate) or large Type I error, depending on the bias in historical data. In the absence of such knowledge of compatibility, how does one choose the value of $\alpha$? In addition, to get around this issue, is it reasonable to treat $\alpha$ itself random and introduce a hyper prior?

See Items 2 and 4 above.

>Some of the Figure and table captions (e.g. Figures 1-5 and Table 1) can benefit from being more descriptive.

We have revised accordingly.

>A couple of notation issues: . . . .

Thank you for pointing out these issues. We have revised Equations (3) and (6) accordingly.

# Reviewer 2

>The method is first illustrated in a simulation study, with a range of values of $\alpha$. This is fine for an audience of statisticians and attorneys specializing in jury discrimination but is somewhat overwhelming for a general reader. It would be easier for the reader if one data set, perhaps from the actual data from Connecticut was used and the reader could see how the data from each additional previous trial impacts the posterior. Perhaps only a few values of $\alpha$, rather than 11 and two val[u]es of the bias parameter b, could be used.

[INSERT]

>From the likelihood on page 4, it is apparent that the method requires data on the consecutive strike patterns, not only in the current case, but in previous ones. How often is this data currently available? If it isn’t readily available now, how likely are states and locales to implement a system of systematically reporting and preserving the required data (especially in the areas where one expects minorities to encounter bias in jury membership)?

In the Discussion, we explained that data on consecutive strike patterns is hard to collect, simply because of the challenges of collecting any historical strike data, regardless of strike procedure. We have revised that Discussion to add that, since the 1980s, any person has the right to attend jury selection in criminal court proceedings These legal rights enable data collection by court observers, regardless of State and local recordkeeping practices. Finally, our statistical approach's validity does not turn on the _current_ availability of the kind of data it requires. Indeed, we hope our statistical approach can inform efforts as to what kinds of data about strikes a court system should make more available.

>In Section 2.3, it is noted that in the simulations, it is assumed that the defense attorney has no bias. If one has been called for jury service in a city with a large minority population and been in a jury room, this is highly implausible. The reviewer once was sitting next to a minority woman and we saw---A minority leave the jury room, then a white, then another minority, then another white etc. At the end both of us noticed the “pattern”. Because one is analyzing a changing pool of possible challenges each time a peremptory challenge is made, when the defense attorney (assuming a minority defendant—the usual situation where Batson issues arise) removes a white, the minority proportion of the pool of possible individuals to be removed increases. This could make it more difficult to detect a biased prosecutor. On the other hand, if the method detects prosecutorial bias when the defense challenges appear biased too, this should strengthen the evidence of prosecutorial bias. This issue deserves some quantitative investigation, so judges can feel confident when using the results.`

[INSERT TABLE]


>Page 5, equation 5: The left-hand side-of the equation is not correct

We have revised to indicate that it is a posterior distribution. All formulas have now been updated and checked. We now specify the vector notation for $\delta_0$ and $\delta_1$.

>Page 9, line 15: The discussion about selecting credible interval percentages is confusing. Are you saying that larger credible interval percentages lead to less accurate bias detection?

See Item 5 above.

>The paper and software focus on peremptory challenges made by the same lawyer in previous cases. As noted in the Miller-El cases, the training manual used by prosecutors recommended the order in which groups, e.g. African-Americans, women etc. should be challenged. Therefore, data from prosecutors from the same DA’s office or defense firm in similar cases (drug dealing, robbery etc.), should be quite informative. Should one simply add them to the list of prior cases, or make a down-weighting adjustment in the Bayesian analysis?

Thank you for this comment.  We have revised the paper to address it. [INSERT]  

Suppose prosecutors from the same office were all exposed to training on how best to strike Black jurors. If so, our approach allows for (but does not require) aggregating all those prosecutors' cases, i.e., acting as if all prosecutors from that office were a single individual prosecutor (e.g., the strike bias of the "DA's office"). Once aggregated, any down-weighting adjustment (choosing $\alpha < 1$) depends on assumptions about how dissimiliarities between the trial-level characteristics (e.g., defendant race, offense type) in the current trial and in past trials may still produce incompatibilty, notwithstanding the prosecutors' common exposure to the training. If none, one could assign equal weight to the historical data ($\alpha$ = 1). If one allows for some incompatibilty nonetheless, one could either subset the historical data (sacrificing power) to include only those past trials that share the same trial-characteristics as the bias-salient characteristics of current trial. Or one could keep all the historical trials; choose an $\alpha$ value to match the degree of suspected incompatibility; and check for how sensitive any bias detection is values of alpha. 

>The paper uses 90% and 95% credible intervals on p. 7, 95% in Tables 2 and 3 and 80% on p. 18 and Fig. 7. From the various contexts, the reviewer got the impression that the author(s) were recommending that 0 not in the 95% credible interval should help the defendant establish their Batson claim, while 0 in an 80% credible interval indicates no bias and would help the prosecutor (in a typical Batson case) rebut a Batson challenge. Is this correct? In any case, the author(s) should indicate which credible intervals should be used at various stages of the proceedings. If possible, the standards for deciding a Batson claim used by judges in a jurisdiction should be the same. This is especially true in the present context where the prior data considered in cases in the same jurisdiction is likely to have a substantial overlap.`

No, this is not correct. We have revised the paper to avoid such confusion. See Item 5 above. We agree that the choice of credible interval should be consistent within different stages of the _Batson_ challenge and across judges deciding _Batson_ challenges within the same jurisdiction.

>Gastwirth (2005) in Law, Prob. Risk, p. 179-185, . . . had proposed that approach when potential jurors were removed one at a time in an alternating sequence. Checking that reference, the reviewer saw that at the end of the 2005
paper, the use of data from prior cases would be helpful and recommended that procedures, such as the one proposed here. This connection should be noted. Also, if one had data from an actual case, one could show the additional insight one obtains by incorporating data from previous cases.

Thank you for this reference. The paper now cites to the model in Gastwirth (2005) as well as a suggested statistical approach in Gastwirth (2014). On "data from an actual case," the discussion about the prototype software application expressly relies on actual strike data from a sample of trials.

>The choice of $\alpha$ should be discussed and recommendations made. Otherwise, both parties can explore which value is most helpful. Also, how can one ensure that the prior data doesn’t overwhelm the data from the current case, without choosing a ridiculously low value? The authors say something about this on page 6 but more guidance to potential users and courts would be helpful.`

On the choice of $\alpha$, see Items 2 and 3 above. At most, the historical strike data and the current strike data have _equal_ weight (when $\alpha$ = 1). We have revised to provide further guidance on these points.

>The scenario depicted in Table 8 and the related text indicates that both the prosecutor and defense removed minority jurors –both 95% credible intervals do not contain 0. This seems unrealistic. More common would be the prosecutor removed more minorities so their 95% credible interval would not contain 0 while the credible interval for the defense removing non-minority potential jurors would be shifted but still might contain 0 (Of course, in some cases when minorities form a substantial portion, e.g. 40% or so of the venire, the data may indicate both sides were probably biased.

Even if it is "[m]ore common" for prosecutors to strike racial minorities, we intend the hypothetical data in Table 8 to underscore that, consistent with _Batson_, nothing in our approach (and in the app) makes different assumptions for prosecutors than for defense attorneys when estimating attorney strike bias.

> At the bottom of p. 17, you don’t really need names for the attorneys, A and B would suffice.

We believe the names make it easier to read and follow along while using the app. The names are fake, courtesy of the `charlatan` package.  Please note that, at the time of initial submission, we offered to make the app prototype available to the reviewers (as an R package), so they too could follow along. We received no reply. The offer remains.
